# Example configuration for small-scale DHRD training
# Useful for quick testing and debugging

model:
  backbone: "gpt2"
  num_labels: 2
  reasoning_weight: 0.5
  pooling_method: "last"
  classification_dropout: 0.1

data:
  train_path: "data/train_small.json"
  val_path: "data/val_small.json"
  max_length: 256
  max_rationale_length: 512
  text_column: "text"
  label_column: "label"
  rationale_column: "rationale"
  include_rationales: true

training:
  num_epochs: 5
  batch_size: 4
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  seed: 42

logging:
  log_interval: 5
  eval_interval: 20
  output_dir: "outputs/dhrd_small"
  use_mlflow: false

device:
  device: "cuda"
